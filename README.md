 **SeNic Data Processing Pipeline**

This repository provides a robust pipeline for processing the raw data from the [SeNic dataset](https://www.google.com/search?q=https://github.com/BoZhuBo/SeNic.git), transforming it from compressed RAR archives into a structured HDF5 file suitable for machine learning applications, particularly with PyTorch.

## **âœ¨ Features**

* **Automated Extraction**: Uncompress RAR archives recursively.  
* **Flexible Preprocessing**: Filter and window time-series data using a configurable YAML file.  
* **Hierarchical Output**: Maintain original directory structure for processed data.  
* **Unified Dataset**: Consolidate all processed data into a single HDF5 file for efficient loading.

## **ðŸš€ Getting Started**

### **Prerequisites**

Before you begin, ensure you have the following installed:

* Python 3.8+  
* unrar command-line tool (install via your system's package manager, e.g., sudo apt-get install unrar on Debian/Ubuntu, brew install unrar on macOS).

### **Installation**

1. **Clone the repository**:  
   git clone https://github.com/leloso/sEMG
   cd sEMG

2. **Install Python dependencies**:  
   pip install \-r requirements.txt

## **ðŸ“– Usage**

Follow these steps to process the SeNic dataset:

### **1\. Acquire Raw Data**

First, download the raw .rar data files from the [SeNic GitHub repository](https://www.google.com/search?q=https://github.com/BoZhuBo/SeNic.git). Place these files in a designated root directory (e.g., raw\_data/).

### **2\. Extract RAR Archives**

Use the unrar\_script.py to extract the contents of the .rar files. This script will recursively search for .rar files and extract them.  
python unrar\_script.py \<directory\_path\> \[--delete\]

* \<directory\_path\>: The root directory where your .rar files are located (e.g., raw\_data).  
* \--delete: (Optional) If present, .rar files will be deleted after successful extraction.

**Example**:  
python unrar\_script.py raw\_data \--delete

### **3\. Preprocess and Window Data**

The preprocess.py script filters and windows the extracted CSV data, saving the processed signals as compressed NPZ files. It replicates the original directory hierarchy in the output directory.  
python preprocess.py \--input\_dir \<input\_directory\> \--output\_dir \<output\_directory\> \--config\_file \<config\_file\_path\>

* \--input\_dir: The root directory containing the extracted CSV files (e.g., raw\_data/extracted).  
* \--output\_dir: The root directory where processed NPZ files will be saved (e.g., processed\_data).  
* \--config\_file: Path to a YAML configuration file specifying preprocessing parameters (e.g., config/preprocessing\_config.yaml).

**Example**:  
python preprocess.py \--input\_dir raw\_data/extracted \--output\_dir processed\_data \--config\_file config/preprocessing\_config.yaml

**Note on Configuration**: The config\_file (e.g., config/preprocessing\_config.yaml) is crucial for defining filtering parameters, window sizes, and other preprocessing specifics. Ensure this file is correctly set up according to your data requirements.

### **4\. Build HDF5 Dataset**

Finally, build\_hdf5.py consolidates all the generated NPZ files into a single, large HDF5 file, which is highly efficient for loading into PyTorch or other deep learning frameworks.  
python build\_hdf5.py \[--data\_dir \<processed\_data\_directory\>\] \[--hdf5\_path \<output\_hdf5\_path\>\]

* \--data\_dir: Directory containing the subject/session/NPZ files generated by preprocess.py (default: processed\_data).  
* \--hdf5\_path: Destination path for the HDF5 file (default: processed\_data/emg\_data.h5).

**Example**:  
python build\_hdf5.py \--data\_dir processed\_data \--hdf5\_path datasets/emg\_data.h5

#### Training Process

The sccript train.py automatically performs training for a given non-ideal factor in the multi-factor analysis
as of now only. For each repetition fold the following are carried out:

1. **Model Selection**: Supports multiple architectures including CNN_GRU, CNN_BiGRU, CNN, CNN_LSTM, TCN, and MESTNet
2. **Data Splitting**: 
   - Uses 2 repetitions for training (with 80-20 train-validation split)
   - Reserves 1 repetition for testing (20% subset for efficiency)
3. **Cross-Validation**: Iterates through all 3 repetitions as test sets (r=0, r=1, r=2)
4. **Model Training**: Each fold trains independently with Adam optimizer and CrossEntropy loss
5. **Results Storage**: Saves trained models and train logs in organized directory structure:
   ```
   <result_dir>/
   â”œâ”€â”€ <experiment_factor>/
   â”‚   â””â”€â”€ <model_name>/
   â”‚       â”œâ”€â”€ r0_final.ckpt
   â”‚       â”œâ”€â”€ r1_final.ckpt
   â”‚       â””â”€â”€ r2_final.ckpt
   ```

#### Configuration Requirements

Your training configuration YAML file must include the following sections:

- **dataset**: Data file path, sequence length, number of classes, channels
- **training**: Learning rate, batch size, split, number of workers
- **optimizer**: Optimization parameters including weight decay
- **model**: Architecture name and specific parameters
- **experiment**: Factor name for organizing results
- **filter**: Subject filtering, positions, sessions, and repetitions
- **validation**: Validation-specific batch size and workers

This cross-validation approach ensures robust evaluation across different data splits, providing reliable performance metrics for comparing model architectures under various non-ideal conditions.

## ðŸ“Š Results Analysis
